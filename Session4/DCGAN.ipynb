{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bvxSPAKQj9gd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] The system cannot find the file specified: '/content'\n",
      "C:\\Users\\alexa\\OneDrive - KU Leuven\\Unief\\Vakken\\2022-2023 master\\Artificial Neural Networks and Deep Learning\\Exercises\\Session4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "fatal: destination path 'gan-tools2' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "%cd /content\n",
    "!rm -rf gan-tools2\n",
    "!git clone --single-branch --depth=1 --branch main https://github.com/hannesdm/gan-tools2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4axTvzA4aI4t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 3] The system cannot find the path specified: '/content/gan-tools2'\n",
      "C:\\Users\\alexa\\OneDrive - KU Leuven\\Unief\\Vakken\\2022-2023 master\\Artificial Neural Networks and Deep Learning\\Exercises\\Session4\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/gan-tools2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cifar10\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimpl\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimpl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\__init__.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\distribute\\__init__.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras' Distribution Strategy library.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sidecar_evaluator\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\distribute\\sidecar_evaluator.py:17\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Python module for evaluation loop.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_logging \u001b[38;5;28;01mas\u001b[39;00m logging\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "%cd /content/gan-tools2\n",
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "import impl\n",
    "from impl import *\n",
    "from core import vis\n",
    "from core import gan\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['axes.grid'] = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqQjkSRHseDK"
   },
   "source": [
    "## Load the cifar10 data\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.  <br/>\n",
    "**Exercise** We will select a single class of this dataset to model. This can be done by setting the **model_class** variable to the corresponding class. <br/>\n",
    "One cell lower, a few images of the selected class are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XbdA5WBEkf05"
   },
   "outputs": [],
   "source": [
    "model_class = 1\n",
    "(X_train_original, Y_train), (_, _) = cifar10.load_data()\n",
    "X_train_single_class = X_train_original[np.where(np.squeeze(Y_train) == model_class)]\n",
    "X_train = X_train_single_class / 127.5 - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHRNCEkA-VA6"
   },
   "outputs": [],
   "source": [
    "grid = vis.image_grid(X_train_single_class[0:20], 5)\n",
    "plt.imshow(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edP72fs0v6bs"
   },
   "source": [
    "## Train the DCGAN\n",
    "<img src=\"https://i.imgur.com/NFUiEf5.png\" width=\"450\"> <br/>\n",
    "The following code will train a GAN with a working DCGAN architecture. This training can be controlled by the following parameters:\n",
    "\n",
    "\n",
    "*   **batches**: The number of batches the GAN should train on.\n",
    "*   **batch_size**: The size of each batch.\n",
    "*    **plot_interval**: After how many batches the generator should be sampled and the images shown.\n",
    "\n",
    "The default parameters may be kept. <br/>\n",
    "Make sure to train the GAN for a sufficient amount of time in order to see realistic samples. At any point, the training may be stopped by clicking on the stop button or on 'interrupt execution' in the runtime menu at the top of the page.<br/> In the same menu, the runtime type should also be changed to 'GPU'. This will speed up the training of the models. <br/>\n",
    "**Exercise** Comment on the loss and accuracy of the generator and discriminator, shown during training and discuss its stability. Explain this in function of the GAN setting.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWgM6KsDmVxU"
   },
   "outputs": [],
   "source": [
    "gan = cifar10_dcgan()\n",
    "gan.train(X_train, steps = 20000, batch_size=32, plot_interval = 200)\n",
    "vis.show_gan_image_predictions(gan, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ev4HeAyrxYJQ"
   },
   "outputs": [],
   "source": [
    "# Plot the final loss curves\n",
    "def moving_average(a, n=10) :\n",
    "    s = np.cumsum(a, dtype=float)\n",
    "    s[n:] = s[n:] - s[:-n]\n",
    "    return s[n - 1:] / n\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "plt.plot(moving_average(gan.d_losses), c=\"blue\", label=\"D Loss\")\n",
    "plt.plot(moving_average(gan.g_losses), c=\"red\", label=\"G Loss\")\n",
    "plt.plot(moving_average(gan.d_accs), c=\"green\", label=\"D Accuracy\")\n",
    "plt.plot(moving_average(gan.g_accs), c=\"yellow\", label=\"G Accuracy\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_s7JNgnDNMq"
   },
   "source": [
    "## Stability in GANs\n",
    "Sadly, training a GAN is not always easy. <br/>\n",
    "Stability during training is important for both discriminator and generator to learn. <br/>\n",
    "Below is a short video (50s) showing the intermediate results of a GAN being trained on mnist. The final result is a phenomenon known as 'mode collapse'. <br/>\n",
    "<img src='https://i.imgur.com/lG35xDP.gif'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PHEJ1xk1TLz"
   },
   "source": [
    "## Optional: High Quality Image Generation with StyleGAN\n",
    "\n",
    "The DCGAN model was an important point in the history of generative adversarial networks. However, these models have difficulty with high resolution images and have long been passed by the current state of the art. </br>\n",
    "State of the art models for high resolution image generation, such as BigGAN and StyleGAN, can generate new images with high fidelity of e.g. 1024x1024 image data sets. The trade-off is that these models can require weeks to train even with the best GPUs and/or TPUs available. </br>\n",
    "</br>\n",
    "**Few-shot learning** A special setting for training generative models is low-shot/few-shot learning where one attempts to create a model that generalizes well on as few samples as possible. This setting allows the power of the state of the art models to be demonstrated while still being able to be trained in a reasonable time. </br>\n",
    "\n",
    "**StyleGAN few-shot**\n",
    "The following script allows you to train a StyleGAN with differentiable augmentations on your own data. Few-shot models work best with uniform, clean data where the object takes up the majority of the image.</br>\n",
    "To the left of Google Colab, click the folder icon on the sidebar, create a new folder in the file explorer and upload your images into it. It's recommended to have at least 100 images of the same width and height. Tip: you can quickly resize all images in a directory using the *imgp* or *convert* commands from a command line.</br> Next, replace the placeholder in the command below with the link to your uploaded folder (e.g. /content/mydata) and execute the command. It's recommended to try out different types of data to see what works and what doesn't.</br>\n",
    "Alternatively, replace the placeholder by the name of one of the pre-existing data sets (e.g. --data=https://hanlab.mit.edu/projects/data-efficient-gans/datasets/100-shot-obama.zip):\n",
    "\n",
    "*   100-shot-obama: https://hanlab.mit.edu/projects/data-efficient-gans/datasets/100-shot-obama.zip\n",
    "*   100-shot-grumpy_cat: https://hanlab.mit.edu/projects/data-efficient-gans/datasets/100-shot-grumpy_cat.zip\n",
    "*   100-shot-panda: https://hanlab.mit.edu/projects/data-efficient-gans/datasets/100-shot-panda.zip\n",
    "*   100-shot-bridge_of_sighs: https://hanlab.mit.edu/projects/data-efficient-gans/datasets/100-shot-bridge_of_sighs.zip\n",
    "*   100-shot-temple_of_heaven: https://hanlab.mit.edu/projects/data-efficient-gans/datasets/100-shot-temple_of_heaven.zip\n",
    "*   100-shot-wuzhen: https://hanlab.mit.edu/projects/data-efficient-gans/datasets/100-shot-wuzhen.zip\n",
    "\n",
    "The script will output intermediate images while training. More full quality samples can be found in the /content/stylegan-lowshot/pytorch/results folder. Take note that it can take multiple hours before reasonable images start to be generated even when working with these very small data sets.\n",
    "Before running the train script below, restart the runtime.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "op_Evd4qW3R2"
   },
   "outputs": [],
   "source": [
    "#%cd /content\n",
    "\n",
    "#!pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "#!pip install ninja\n",
    "\n",
    "#! rm -rf stylegan-lowshot\n",
    "#!git clone --single-branch --depth=1 --branch main https://github.com/hannesdm/stylegan-lowshot.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dCd40oU1SDs"
   },
   "outputs": [],
   "source": [
    "#%cd /content/stylegan-lowshot/pytorch/\n",
    "#%run train.py --outdir=results --data=https://data-efficient-gans.mit.edu/datasets/100-shot-wuzhen.zip --gpus=1 --snap=1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DCGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
